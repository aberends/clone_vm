TINY_KVM_FOR_CLONING


OBJECTIVE

Explain how to create a tiny KVM for cloning usage.


AUTHOR

Allard Berends (AB)


HISTORY

Date with 'LC_TIME=en_US date +"\%d-\%b-\%Y \%H:\%M", '.
01-Apr-2019 20:19, AB, start


REFERENCES

[ad_plan]         Active directory, ~/ad/plan.txt

[dc_plan]         Docker, ~/docker/plan.txt

[gl_plan]         Gitlab installation and use,
                  ~/gitlab/plan.txt

[jq_plan]         jq usage, ~/jq/plan.txt

[kc_plan]         Keycloak installation, ~/keycloak/plan.txt

[kvm_wp]          Kernel-based Virtual Machine,
                  https://en.wikipedia.org/wiki/Kernel-based_Virtual_Machine

[lv_conn_uris]    libvirt: Connection URIs,
                  https://libvirt.org/uri.html

[nvme_tour]       A Quick Tour of NVM Express (NVMe),
                  https://metebalci.com/blog/a-quick-tour-of-nvm-express-nvme/

[pni_plan]        Patroni HA PostgreSQL, ~/patroni/plan.txt

[rhce_plan]       Preparing for RHCE exam, ~/rhce/plan.txt


ABBREVIATIONS

KVM               Kernel-based Virtual Machine
VG                Volume Group


TERMS

KVM               Kernel-based Virtual Machine (KVM) is a
                  virtualization module in the Linux kernel
                  that allows the kernel to function as a
                  hypervisor. More information on [kvm_wp].


SECTIONS

Create with:
- delete current entries
- save file
- exter "ex" mode by typing ':'
- r !sed -n 's/^\([A-Z0-9][A-Z_0-9]*\)$/- \1/p' %:p

- TINY_KVM_FOR_CLONING
- OBJECTIVE
- AUTHOR
- HISTORY
- REFERENCES
- ABBREVIATIONS
- TERMS
- SECTIONS
- IP_PLAN
- NVME
- KVM_HOST_PREREQUISITES
- RPMS_AND_GROUPS_PREREQUISITES
- VIRTUALIZATION_SERVICE
- SETUP_POOL
- ENABLE_VIRSH_NORMAL_USER
- ENABLE_VIRT_MANAGER
- DOWNLOAD_ISO
- VIRT_INSTALL
- SHRINK_VM_DISK
- UNATTENDED_CONFIGURATION_WITH_ANSIBLE


IP_PLAN

We derive our local IP plan from the mcare-m environment in
AWS. We obtain the information from AWS as described in
section "AWS_EC2_NODES" of [jq_plan]. Note, the alias
mcare_m includes the information as in section
"AWS_EC2_NODES".

$ mcare_m | awk -F, '{print $2 " " $1}' | sort -t '.' -k 4,4 -n
10.200.16.10 mcare-m-rhc001
10.200.16.11 mcare-m-idm001
10.200.17.11 mcare-m-idm002
10.200.16.12 mcare-m-zbx001
10.200.16.13 mcare-m-lgs001
10.200.16.16 mcare-m-zbp001
10.200.17.16 mcare-m-zbp002
10.200.16.22 mcare-m-ssh001
10.200.17.22 mcare-m-ssh002
10.200.16.24 mcare-m-sec001
10.200.17.24 mcare-m-lgs002
10.200.16.25 mcare-m-els001
10.200.17.25 mcare-m-els002
10.200.17.26 mcare-m-kfz002
10.200.16.28 mcare-m-mgt001
10.200.16.29 mcare-m-kfz001
10.200.19.30 mcare-m-jen001
10.200.16.32 mcare-m-pup001
10.200.16.38 mcare-m-rnd001

Since we only want single instances of every node type we
grep for 001. So, we get:

$ mcare_m | grep 001 | awk -F, '{print $2 " " $1}' | sort -t '.' -k 4,4 -n
10.200.16.10 mcare-m-rhc001
10.200.16.11 mcare-m-idm001
10.200.16.12 mcare-m-zbx001
10.200.16.13 mcare-m-lgs001
10.200.16.16 mcare-m-zbp001
10.200.16.22 mcare-m-ssh001
10.200.16.24 mcare-m-sec001
10.200.16.25 mcare-m-els001
10.200.16.28 mcare-m-mgt001
10.200.16.29 mcare-m-kfz001
10.200.19.30 mcare-m-jen001
10.200.16.32 mcare-m-pup001
10.200.16.38 mcare-m-rnd001

Now, we want to transform it to the default network of
libvirt, which is 192.168.122.0/24 and also the FQDNs to
something.local.lan:

$ mcare_m | grep 001 | awk -F, '{gsub(/.*-/, "", $1); gsub(/.*\./, "", $2); printf "%-15s  %s\n", "192.168.122."$2, "nuc3-m-"$1".tux.m.nuc3.lan nuc3-m-"$1}' | sort -t. -k 4,4 -n
192.168.122.10   nuc3-m-rhc001.tux.m.nuc3.lan nuc3-m-rhc001
192.168.122.11   nuc3-m-idm001.tux.m.nuc3.lan nuc3-m-idm001
192.168.122.12   nuc3-m-zbx001.tux.m.nuc3.lan nuc3-m-zbx001
192.168.122.13   nuc3-m-lgs001.tux.m.nuc3.lan nuc3-m-lgs001
192.168.122.16   nuc3-m-zbp001.tux.m.nuc3.lan nuc3-m-zbp001
192.168.122.22   nuc3-m-ssh001.tux.m.nuc3.lan nuc3-m-ssh001
192.168.122.24   nuc3-m-sec001.tux.m.nuc3.lan nuc3-m-sec001
192.168.122.25   nuc3-m-els001.tux.m.nuc3.lan nuc3-m-els001
192.168.122.28   nuc3-m-mgt001.tux.m.nuc3.lan nuc3-m-mgt001
192.168.122.29   nuc3-m-kfz001.tux.m.nuc3.lan nuc3-m-kfz001
192.168.122.30   nuc3-m-jen001.tux.m.nuc3.lan nuc3-m-jen001
192.168.122.32   nuc3-m-pup001.tux.m.nuc3.lan nuc3-m-pup001
192.168.122.38   nuc3-m-rnd001.tux.m.nuc3.lan nuc3-m-rnd001

Note, for the nuc3 itself on which we run the VMs we use the
wifi network and a cross cable when directly connected to
nuc1.

For this purpose we set:

nuc3# nmcli connection modify eno1 ipv4.addresses 192.168.2.3/24 ipv4.method manual
nuc3# nmcli connection up eno1

On the nuc1 this also needs to be set, so:

nuc1# nmcli connection modify eno1 ipv4.addresses 192.168.2.1/24 ipv4.method manual
nuc1# nmcli connection up eno1

To access the VMs on nuc3 from the command line on nuc1, we
configure ssh with the following settings in ~/.ssh/config:

$ mcare_m | grep 001 | awk -F, '{gsub(/.*-/, "", $1); print "nuc3-m-"$1}' | while read line
do
  echo -e "Host $line\n  Hostname $line\n  StrictHostKeyChecking no\n  UserKnownHostsFile=/dev/null\n  ProxyJump nuc3lan\n  User root\n"
done
Host nuc3-m-els001
  Hostname nuc3-m-els001
  StrictHostKeyChecking no
  UserKnownHostsFile=/dev/null
  ProxyJump nuc3lan
  User root
.
.

Prepend with:

# Nuc3
Host nuc3lan
  Hostname 192.168.2.3
  User allard

For Ansible, see [rhce_plan]:
192.168.122.40   nuc3-m-acn001.tux.m.nuc3.lan nuc3-m-acn001

For Windows, see [ad_plan]:
192.168.122.50   nuc3-m-mws001.tux.m.nuc3.lan nuc3-m-mws001
192.168.122.51   nuc3-m-mwc001.tux.m.nuc3.lan nuc3-m-mwc001

For keycloak, see [kc_plan]:
192.168.122.60   nuc3-m-kcs001.tux.m.nuc3.lan nuc3-m-kcs001

For gitlab, see [gl_plan]:
192.168.122.70   nuc3-m-gls001.tux.m.nuc3.lan nuc3-m-gls001

For docker swarm, see [dc_plan]:
192.168.122.80   nuc3-m-dsm001.tux.m.nuc3.lan nuc3-m-dsm001
192.168.122.81   nuc3-m-dsm002.tux.m.nuc3.lan nuc3-m-dsm002
192.168.122.82   nuc3-m-dsm003.tux.m.nuc3.lan nuc3-m-dsm003

For patroni, see [pni_plan]:
192.168.122.90   nuc3-m-pni001.tux.m.nuc3.lan nuc3-m-pni001
192.168.122.91   nuc3-m-pni002.tux.m.nuc3.lan nuc3-m-pni002
192.168.122.93   nuc3-m-etc001.tux.m.nuc3.lan nuc3-m-etc001
192.168.122.94   nuc3-m-etc002.tux.m.nuc3.lan nuc3-m-etc002
192.168.122.95   nuc3-m-etc003.tux.m.nuc3.lan nuc3-m-etc003
192.168.122.96   nuc3-m-hap001.tux.m.nuc3.lan nuc3-m-hap001
192.168.122.97   nuc3-m-hap002.tux.m.nuc3.lan nuc3-m-hap002


NVME

The disk we use on the host is a M.2 SSD. The communication
is done via a protocol called NVMe. It sits between the PCIe
slot and the controller on the disk. For an interesting
read, refer to [nvme_tour].

The name of the driver in Linux is nvme. The /dev/nvme0
refers to the first controller found. A controller deals
with the storage units. These are divided in so-called
namespaces, e.g. /dev/nvme0n1 is the first namespace on the
first nvme controller.

The namespaces provide space for a block device that can be
partitioned, e.g. /dev/nvme0n1p1 is the first partition on
the first namespace on the first nvme controller.

On Linux you can see it with:

# lsblk -i
NAME            MAJ:MIN RM   SIZE RO TYPE MOUNTPOINT
nvme0n1         259:0    0 894.3G  0 disk
|-nvme0n1p1     259:1    0   600M  0 part /boot/efi
|-nvme0n1p2     259:2    0     1G  0 part /boot
`-nvme0n1p3     259:3    0 892.7G  0 part
  |-fedora-root 253:0    0 884.7G  0 lvm  /
  `-fedora-swap 253:1    0     8G  0 lvm  [SWAP]


KVM_HOST_PREREQUISITES

We tested our cloning practise on a CentOS7 and a Fedora29
host. The host is installed with a Minimal installation
source and manual partitioning is chosen with the default
partitioning scheme. The sizes used are:

- boot: 1024 MiB
- swap: 8 GiB (internal memory is 32 GiB)
- root: 30 GiB

Because of the default partitioning scheme we have at least
the centos (CentOS7 case) or the fedora (Fedora29 case) VG
(Volume Group).

The installer limits the partition size on which the centos
or fedora VG resides to the size needed. Since we have a 960
GiB disk we have more space. Consequently we need to grow
the partition, the PV and VG:

# parted -s /dev/nvme0n1 resizepart 3 100%

Note that the partition number can be different if you used
legacy boot. Since we used UEFI boot we have an extra
partition and we end up resizing partition number 3. With
legacy boot it is partition number 2.

Now resize the PV:

# pvresize /dev/nvme0n1p3
  Physical volume "/dev/nvme0n1p3" changed
  1 physical volume(s) resized or updated / 0 physical volume(s) not resized

This can be verified with:

c7# pvs
  PV         VG      Fmt  Attr PSize   PFree   
  /dev/sda2  centos  lvm2 a--  697.63g <659.64g

f29# pvs
  PV             VG     Fmt  Attr PSize    PFree   
  /dev/nvme0n1p3 fedora lvm2 a--   893.05g <549.85g

Make sure that on the VG at least 10GiB is available to make
the cloneable VM.

We want to run with the latest updates, so, before we do
anything else, we execute:

#
yum -y update
reboot

TODO: document how to use up the VG so that PFree is 0.


RPMS_AND_GROUPS_PREREQUISITES

TODO: update the text in this section based on the
information obtained within a clean installation on a
CentOS7 and a Fedora29 host.

We assume that the host (CentOS7 or Fedora29) is installed
with a Minimal installation source. So, before we can run
the create_tpl_vm.sh script which calls ansible-playbook, we
need to install ansible:

#
yum -y install epel-release   # Only for CentOS7.
yum -y install ansible

In order to obtain the software to make the cloneable VM, we
need git too:

# yum -y install git

Now we install the software under a normal user account
called "allard" (change with your name if you like). So we
need to add a user. Since the user needs to be able to
execute privileged commands later, we also add the user to
the wheel group and make sure it can execute as root without
providing a password:

#
useradd allard
echo redhat | passwd --stdin allard
usermod -a -G wheel allard
sed -i -e 's/^\(%wheel[ \t]\+ALL=(ALL)[ \t]\+ALL\)$/#\1/' -e 's/^# \(%wheel[ \t]\+ALL=(ALL)[ \t]\+NOPASSWD:[ \t]\+ALL\)$/\1/' /etc/sudoers

Log out from the root account and login as "allard". Then
execute:

$
git clone https://github.com/aberends/clone_vm.git

Next we execute the script:

$
cd clone_vm
./create_tpl_vm.sh

We use the following commands:

- ansible
- ansible-playbook
- guestfish
- jinja2
- virsh
- virt-clone
- virt-manager
- virt-resize
- virt-xml
- wget

In the CentOS7 yum repositories a number of groups related
to virtualization exist. They can be found with the
following command:

# yum group list ids hidden | grep -i virtualization
   Virtualization Host (virtualization-host-environment)
   Virtualization Hypervisor (virtualization-hypervisor)
   Virtualization Tools (virtualization-tools)
   Virtualization Client (virtualization-client)
   Virtualization Platform (virtualization-platform)

We install the groups with:

# yum -y group install virtualization-host-environment virtualization-hypervisor virtualization-tools virtualization-client virtualization-platform

We check if we have all the commands we need:

# for i in guestfish virsh virt-clone virt-manager virt-resize virt-xml; do which $i; done
/usr/bin/which: no guestfish in (/usr/local/sbin:/sbin:/bin:/usr/sbin:/usr/bin:/root/bin)
/bin/virsh
/bin/virt-clone
/bin/virt-manager
/usr/bin/which: no virt-resize in (/usr/local/sbin:/sbin:/bin:/usr/sbin:/usr/bin:/root/bin)
/bin/virt-xml

So we are missing guestfish and virt-resize commands. In
which packages do these commands reside?

# yum provides '*/guestfish'
.. truncated ..
libguestfs-tools-c

BTW, in the output we also see the package
libguestfs-bash-completion, which is of interest.

# yum provides '*/virt-resize'
.. truncated ..
libguestfs-tools-c

So, do these packages reside in groups of interest? To query
yum groups for packages we first need to install a yum
plugin:

# yum -y install yum-plugin-list-data

Now we can query for the packages to find out if they reside
in yum groups:

# yum list-groups libguestfs-tools-c 
Loaded plugins: fastestmirror, langpacks, list-data
Loading mirror speeds from cached hostfile
 * base: mirror.yourwebhoster.eu
 * epel: ams.edge.kernel.org
 * extras: centos.mirror.triple-it.nl
 * updates: centos.mirror.triple-it.nl
==================== Available Packages ====================
Virtualization Client      1 ( 50%)
Virtualization Tools       1 ( 50%)
list-groups done

We already installed both groups. So why don't we have
libguestfs-tools-c? We look for information about the groups
with:

# yum group info 'Virtualization Client'
# yum group info 'Virtualization Tools'

It turns out that libguestfs-tools-c is listed as optional
in both 'Virtualization Client' and 'Virtualization Tools'.

We install the optional packages of both groups with:

# yum --setopt=group_package_types=optional -y group install 'Virtualization Client' 'Virtualization Tools'

Alas, the command does not work. Even setting the following
line in our "[main]" section of yum.conf does not work:

# grep group_package_types /etc/yum.conf
group_package_types=default, mandatory, optional

So, we install the package with a normal install:

yum -y install libguestfs-tools-c

In order to do some automatic configuration, we also need
ansible:

yum -y install ansible

For testing of jinja2 templates (used by Ansible) on the
command line, we install:

yum -y install python2-pip
pip install jinja2-cli

TODO: RHEL8 and Fedora32 use python 3. Document how to
install it.


VIRTUALIZATION_SERVICE

In order to use KVM, the virtualization service must run.
Hence we configure:

#
systemctl enable libvirtd.service
systemctl start libvirtd.service
systemctl is-active libvirtd.service
# shows "active"
systemctl is-enabled libvirtd.service
# shows "enabled"

Make sure the default network is setup and starts at boot
time:

# virsh net-list --autostart
 Name                 State      Autostart     Persistent
----------------------------------------------------------
 default              active     yes           yes

To figure out the IPv4 range of the default network:

# virsh net-dumpxml default
<network>
  <name>default</name>
  <uuid>19828e4a-4081-4a40-8717-cc046cb82e3a</uuid>
  <forward mode='nat'>
    <nat>
      <port start='1024' end='65535'/>
    </nat>
  </forward>
  <bridge name='virbr0' stp='on' delay='0'/>
  <mac address='52:54:00:3d:26:fc'/>
  <ip address='192.168.122.1' netmask='255.255.255.0'>
    <dhcp>
      <range start='192.168.122.2' end='192.168.122.254'/>
    </dhcp>
  </ip>
</network>

To see the status of the virtual bridge of the default
network:

# ip -4 addr show dev virbr0
4: virbr0: <NO-CARRIER,BROADCAST,MULTICAST,UP> mtu 1500 qdisc noqueue state DOWN group default qlen 1000
    inet 192.168.122.1/24 brd 192.168.122.255 scope global virbr0
       valid_lft forever preferred_lft forever

The NO-CARRIER means that the bridge is not connected to a
medium.

Physical network interfaces have a different set of flags.
Notably the LOWER_UP flag. See netdevice(7). It means that a
cable is connected to the ethernet port. For example:

# ip -4 addr show dev enp0s31f6
2: enp0s31f6: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc pfifo_fast state UP group default qlen 1000
    inet 192.168.178.62/24 brd 192.168.178.255 scope global noprefixroute dynamic enp0s31f6
       valid_lft 2686sec preferred_lft 2686sec

The virb0 device is a virtual device. Hence it cannot be
connected to a cable and show the state LOWER_UP. The
states NO-CARRIER and LOWER_UP are opposites.

Q.  After a reboot of the host VMs receive the same MAC as
    before. Apparently the MAC is persistently stored by
    libvirtd. But where?
A.  The file /var/lib/libvirt/dnsmasq/virbr0.macs contains
    the MAC addresses of the VMs that have an interface on
    virbr0. It is in JSON format. To use it with jq, see
    below.

We want to find the domain that holds the MAC address
52:54:c0:a8:7a:82. We use the following command:

$ jq '.[] | select(any(.macs[] == "52:54:c0:a8:7a:82"; .))' /var/lib/libvirt/dnsmasq/virbr0.macs
{
  "domain": "edb001",
  "macs": [
    "52:54:c0:a8:7a:82"
  ]
}


SETUP_POOL

Note, this section is only relevant if you want to run the
pool as a VG.

#
virsh pool-define-as centos logical /dev/centos
virsh pool-autostart centos
virsh pool-start centos


ENABLE_VIRSH_NORMAL_USER

We want to run all command line virsh commands from within a
normal account. Instead of having to type "--connect
qemu://system", we can set it as default for the user:

$
mkdir -p ~/.config/libvirt
cat << '_EOF_' > ~/.config/libvirt/libvirt.conf
uri_default = "qemu:///system"
_EOF_

This information comes from [lv_conn_uris], by compiling the
information in section "Configuring URI aliases"
(libvirt.conf) and section "Default URI choice".


ENABLE_VIRT_MANAGER

From another machine we use SSH to log in on our laptop
running KVM with the libvirtd service. Under a standard
user, we try to run virt-manager:

$ virt-manager
$
(virt-manager:29390): Gtk-WARNING **: 14:27:03.938: cannot open display:
^C

To solve this issue, log in over SSH with the "-X" flag. The
first time we get the warning:

/usr/bin/xauth:  file /home/allard/.Xauthority does not exist

We can ignore it. It means that xauth has created the
.Xauthority file for the user.

Now we use virt-manager again and it works but gives a
warning. What does the warning mean?

If we use virt-manager on the laptop directly, a security
pop-up modal window is shown. The word "modal" means that
the pop-up window takes over control. So either you provide
the information or you cancel the operation, but you can not
do anything else on the GUI. This is what a modal window
means.

Being logged in over SSH means that the policy-kit cannot
pop-up the modal window and ask the user for a password. So,
we need to tell policy-kit to allow the user to run
virt-manager. It is done like this:

# usermod -a -G libvirt allard

Now the user allard can run virt-manager directly on the
laptop withouth having to answer the pop-up window.
Furthermore, since the pop-up is not launched any longer,
the user can also start virt-manager over SSH with the "-X"
for X11 forwarding option.


DOWNLOAD_ISO

In this section we describe how to setup a CentOS7 template.
This template is used to clone from to create a new Virtual
Machine.

The CentOS7 template is a CentOS7 VM that is shrunk after
installation. The first step is to install a VM.

We need the CentOS7 minimal installation ISO for this task:

$
# Go to clone_vm inside the home directory of the user.
cd ~/clone_vm
# Obtain the ISO redirect URL from CentOS for the minimal
# install.
#URL=$(curl -s https://www.centos.org/download/ | grep -o 'http://.*-x86_64-Minimal-.*\.iso')
URL="http://ftp.tudelft.nl/centos.org/7.8.2003/isos/x86_64/CentOS-7-x86_64-Minimal-2003.iso"
# Obtain the ISO file.
ISOFILE=$(echo $URL | sed 's#.*/##')
# Check if the ISO is already downloaded in the home
# directory of the user. If not, remove old ISO's and
# download new one.
[ -f "~/Downloads/$ISOFILE" ] || { rm -f *-x86_64-Minimal-*.iso; wget -P ~/Downloads $URL; }

During the installation virt-manager shows a pop-up with the
question: The emulator may not have search permissions for
the path ... Do you want to correct this now?

If you say yes, the installer executes setfacl to correct
the permissions. We do it on the command line to avoid this
pop-up altogether:

$
cd ~/Downloads
setfacl -m u:qemu:x .

Note, if you want to experiment with the ACL, it can be
removed with:

$ setfacl -x u:qemu: .

Calculate the MAC address:

$ printf "52:54:%2.2x:%2.2x:%2.2x:%2.2x\n" 192 168 122 3
52:54:c0:a8:7a:03

Note, when we do an install we also get the warning:

WARNING  /home/allard/.cache/virt-manager/boot may not be accessible by the hypervisor. You will need to grant the 'qemu' user search permissions for the following directories: ['/home/allard/.cache']

TODO: explain how to fix this.


VIRT_INSTALL

The descriptions of the node types are obtained from what we
set in terraform. So, we use:

$ cd terraform-mcare-care
$ grep -e '^module' -e Description aws_ec2.tf

From the output we can deduce what description we want to
give to an instance. Since we use each node type only once
we don't need to add a sequence number.

The description for the Zabbix server is for example:
"Zabbix". For the Zabbix proxy it is: "Zabbix Proxy".

Obtain possible OS variants that can be used in KVM:

# osinfo-query os | grep -i centos
.. centos7.0 is what we use ..

All variables we use:

OS_VARIANT="centos7.0"
VG="os"
DOMAIN=tpl004
SIZE_MIB=$((5*1024))
NET_PART="192 168 122"
NET_NAME="default"
OCTET="4"
MEMORY_MIB=1024
DESCRIPTION="CentOS7 template"
# Assumption is that we only have 1 Minimal CentOS7 ISO.
ISO_PATH=/home/allard/clone_vm/CentOS-7-x86_64-Minimal-*.iso
KICKSTART_PATH=/home/allard/clone_vm/$DOMAIN.ks
MAC=$(printf "52:54:%2.2x:%2.2x:%2.2x:%2.2x" $NET_PART $OCTET)

Prior to installation a storage volume for the VM is needed.
We give it the name of the domain, i.e. tpl004 and the size
of 5 GiB. Older installations of the VM are completely
removed:

#
virsh destroy $DOMAIN
virsh undefine $DOMAIN
virsh vol-delete $DOMAIN $VG
virsh vol-create-as $VG $DOMAIN ${SIZE_MIB}m

The kickstart file is:

$ cat << _EOF_ > $KICKSTART_PATH
# System authorization information
auth --enableshadow --passalgo=sha512
# Use CDROM installation media
cdrom
# Add CentOS7 base repo. Note, the variables can be
# discovered by using the following command on a running
# CentOS7 system:
# yum --setopt=ui_repoid_vars=id,releasever,basearch,infra repolist
#repo --name=base --mirrorlist=http://mirrorlist.centos.org/?release=\$releasever&arch=\$basearch&repo=os&infra=\$infra
repo --name=base --mirrorlist=http://mirrorlist.centos.org/?release=7&arch=x86_64&repo=os&infra=stock
# Use text install.
text
# Don't run the Setup Agent on first boot.
firstboot --disable
ignoredisk --only-use=vda
# Keyboard layouts
keyboard --vckeymap=us --xlayouts='us'
# System language
lang en_US.UTF-8

# Network information
network --bootproto=static --device=eth0 --gateway=192.168.122.1 --ip=192.168.122.$OCTET --nameserver=8.8.4.4,8.8.8.8 --netmask=255.255.255.0 --ipv6=auto --activate
network --hostname=$DOMAIN.home.org

# Root password
rootpw --iscrypted \$6\$VqpnR1p7fX77VP1I\$y2bB8RshiFXMAgzHed4RIaZUR1ny8GnXGCCw8uHRItsvx/xsnqsx0X/YMwIuRfKKmBQ5FCeTUkP9mnXDzri9u1
# System services
services --enabled="chronyd"
# System timezone
timezone Europe/Amsterdam --isUtc --nontp
# System bootloader configuration.
zerombr
bootloader --append=" crashkernel=auto" --location=mbr --boot-drive=vda
# Partition clearing information
clearpart --drives=vda --all --initlabel
# Disk partitioning information
# AB: set the boot partition to a fixed size. 512 MiB is
# more than enough for initial CentOS7 installations.
part /boot --fstype="xfs" --ondisk=vda --size=512
# AB: the remainder of the disk is dedicated to the os VG.
part pv.157 --fstype="lvmpv" --ondisk=vda --size=1 --grow
volgroup $VG --pesize=4096 pv.157
logvol swap --fstype="swap" --size=511 --name=swap --vgname=$VG
# AB: the remainder of the VG is dedicated to the root
# filesystem.
logvol / --fstype="xfs" --size=1 --grow --name=root --vgname=$VG
# AB: poweroff the system to give us a change to
# persistently remove the CDROM.
poweroff

# AB: the minimum packages specification (@^minimal, @core
# and kexec-tools) is enlarged with packages we always need.
%packages
@^minimal
@core
kexec-tools
bash-completion
mlocate
tcpdump
telnet
tree
vim-enhanced
wget
xfsdump
yum-utils
%end

%addon com_redhat_kdump --enable --reserve-mb='auto'
%end

%post --log=/root/ks-post.log
echo post log test
%end

%anaconda
pwpolicy root --minlen=6 --minquality=1 --notstrict --nochanges --notempty
pwpolicy user --minlen=6 --minquality=1 --notstrict --nochanges --emptyok
pwpolicy luks --minlen=6 --minquality=1 --notstrict --nochanges --notempty
%end
_EOF_

Information is given in virt-install(1). The options are in
the order of virt-install(1).

virt-install \
  --connect qemu:///system \
  --name=$DOMAIN \
  --memory=$MEMORY_MIB \
  --arch=x86_64 \
  --metadata=description="$DESCRIPTION" \
  --vcpus=vcpus=1 \
  --location=$ISO_PATH \
  --extra-args="ks=file:/$DOMAIN.ks console=ttyS0,115200 inst.sshd" \
  --initrd-inject=$KICKSTART_PATH \
  --os-variant="$OS_VARIANT" \
  --boot=hd \
  --disk=vol=os/$DOMAIN,device=disk,bus=virtio \
  --network=network=$NET_NAME,mac=$MAC \
  --graphics=none \
  --noautoconsole \
  --hvm \
  --autostart

Note, the option "--noautoconsole" tells virt-install not to
connect to the console. This way we stay disconnected from
the VM.

In the option "--extra-args" we specify "inst.sshd" to
enable an sshd during installation. You can connect to it as
root without password using the IPv4 address configured in
the kickstart file.

We need to wait until the installation is finished. We can
detect the completion of the installation with the command:

# watch -n 1 "virsh domstate $DOMAIN"
.. "running" during installation ..
.. "shut off" after installation ..

After the installation we want to remove the CDROM drive. To
accomplish this, we use:

# virt-xml $DOMAIN --remove-device --disk device=cdrom
Domain 'tpl004' defined successfully.

The "--autostart" start option was used. But for some reason
it does not mark the VM for autostart. When we don't use the
option "--noautoconsole" the option "--autostart" does work.
Anyway, we need to set it to autostart again:

# virsh autostart $DOMAIN

To start the VM:

# virsh start $DOMAIN


VIRSH_VOL

The pools:

nuc3$ virsh pool-list --name
boot-scratch
default
Downloads

nuc3$ virsh pool-dumpxml default
<pool type='dir'>
  <name>default</name>
  <uuid>8cd53589-93a3-48cd-9e4a-27d3eb17519e</uuid>
  <capacity unit='bytes'>949436985344</capacity>
  <allocation unit='bytes'>686925803520</allocation>
  <available unit='bytes'>262511181824</available>
  <source>
  </source>
  <target>
    <path>/var/lib/libvirt/images</path>
    <permissions>
      <mode>0711</mode>
      <owner>0</owner>
      <group>0</group>
      <label>system_u:object_r:virt_image_t:s0</label>
    </permissions>
  </target>
</pool>

nuc3$ virsh vol-list default | grep -e rh8 -e Name
 Name                                                    Path
 rh8001.qcow2                                            /var/lib/libvirt/images/rh8001.qcow2
 rh8002.qcow2                                            /var/lib/libvirt/images/rh8002.qcow2
 rh8003.qcow2                                            /var/lib/libvirt/images/rh8003.qcow2

nuc3$ virsh vol-info --pool default rh8002.qcow2
Name:           rh8002.qcow2
Type:           file
Capacity:       20.00 GiB
Allocation:     2.41 GiB

nuc3$ virsh vol-create-as default rh8002_vdb.qcow2 2g
Vol rh8002_vdb.qcow2 created

nuc3$ virsh vol-info --pool default rh8002_vdb.qcow2
Name:           rh8002_vdb.qcow2
Type:           file
Capacity:       2.00 GiB
Allocation:     2.00 GiB

nuc3$ virsh attach-disk rh8002 /var/lib/libvirt/images/rh8002_vdb.qcow2 vdb
Disk attached successfully

nuc3$ virsh detach-disk rh8002 /var/lib/libvirt/images/rh8002_vdb.qcow2
Disk detached successfully


SHRINK_VM_DISK

What if we create the disk for the VM via lvcreate?

We do a test:

# lvcreate -n tst001 os -L 5g
  Logical volume "tst001" created.

# lvs os/tst001
  LV     VG Attr       LSize Pool Origin Data%  Meta%  Move Log Cpy%Sync Convert
  tst001 os -wi-a----- 5.00g

# virsh vol-list os
.. no tst001 in the output ..

So, we need to create the disk via "virsh vol-create-as".

Next, we create a data disk on the host and attach it to the
VM:

#
# AB: note 2GiB (2g) is assumed to be more than enough to
# contain the XFS dumps.
virsh vol-delete data $VG
virsh vol-create-as $VG data 2g
virsh attach-disk $DOMAIN /dev/$VG/data vdb

Log in on the VM:

# ssh -l root 192.168.122.4

Create filesystem, mount it and dump the other filesystems
on it:

#
mkfs.xfs -f /dev/vdb
mkdir /mnt/data
mount /dev/vdb /mnt/data
xfsdump -L '' -M '' -f /mnt/data/rootfs /
xfsdump -L '' -M '' -f /mnt/data/bootfs /boot

Calculate the sizes expressed in MiB (--block-size=1M)

#
BOOTSIZE=$(ls -l --block-size=1M /mnt/data/bootfs | awk '{ print $5 }')
ROOTSIZE=$(ls -l --block-size=1M /mnt/data/rootfs | awk '{ print $5 }')
SIZE=$(( $BOOTSIZE + 20 + $ROOTSIZE + 100 ))

How to figure out to what VM a previous tpl LV is attached?

#
TPL_ATTACHED=$(virsh list --name --all | head -n -1 | while read line; do dummy=$(virsh domblklist $line | tail -n +3 | head -n -1 | grep tpl); [ -n "$dummy" ] && echo "$line $dummy"; done)
[ -n "$TPL_ATTACHED" ] && virsh detach-disk ${TPL_ATTACHED%% *} ${TPL_ATTACHED##* }

On the host create a new disk of the calculated size:

# virsh vol-create-as $VG tpl ${SIZE}M

Attach the new (small) disk to the VM:

# virsh attach-disk $DOMAIN /dev/os/tpl vdc

Log in, partition the new disk and put filesystems on the
partitions:

#
parted -s /dev/vdc "unit MiB mklabel msdos mkpart primary 0% ${BOOTSIZE}MiB set 1 boot on mkpart primary ${BOOTSIZE}MiB 100%"
mkfs.xfs -f /dev/vdc1
pvcreate --yes -ff /dev/vdc2
vgcreate tpl /dev/vdc2
lvcreate --yes -n swap -l 2 tpl
lvcreate --yes -n root -l 100%VG tpl
mkfs.xfs -f /dev/tpl/root

Now, mount the new filesystems and restore the original boot
and root filesystems on them:

#
mkdir /tmp/{boot,root}
mount /dev/tpl/root /tmp/root
xfsrestore -f /mnt/data/rootfs /tmp/root
mount /dev/vdc1 /tmp/root/boot
xfsrestore -f /mnt/data/bootfs /tmp/root/boot
mount --bind /dev /tmp/root/dev
mount --bind /dev/pts /tmp/root/dev/pts
mount --bind /proc /tmp/root/proc
mount --bind /sys /tmp/root/sys
chroot /tmp/root

Correct information to make the shrunk disk boot:

#
sed -i -e 's#=[^ ]*/root#=tpl/root#' \
       -e 's#=[^ ]*/swap#=tpl/swap#' /etc/default/grub
grub2-install /dev/vdc
grub2-mkconfig > /boot/grub2/grub.cfg
BOOT_UUID=$(blkid /dev/vdc1 -o value -s UUID)
sed -i -e "s/UUID=[^ ]\+ /UUID=$BOOT_UUID /" \
       -e 's#/[^-/]\+-root#/tpl-root#' \
       -e 's#/[^-/]\+-swap#/tpl-swap#' /etc/fstab
#sed -i -e 's/^orig7.home.org/tpl7.home.org/' /tmp/root/etc/hostname
mkswap /dev/tpl/swap


UNATTENDED_CONFIGURATION_WITH_ANSIBLE
